}
normm<-function (Nsim, a) # give the n. of simulations and the a rate. that is the acceptance rate.
{       vec <- vector("numeric", Nsim) # create an vector where the simulation results are stored
x <- 0 # Init value
vec[1] <- x # put init value to the vector that stores the resultsa
for (i in 2:Nsim) { # start the for loop that goes from 2 to the Nsim
innov <- runif(1, -a, a) # sample from uniform[-a, a]
Xstar <- x + innov # proposal plus random walk element innov
aprob <- min(1, dnorm(Xstar)/dnorm(x)) # aprov rate
u <- runif(1) # sample the minumum that must be cleared, so that the proposal is accepted.
if (u < aprob)  # if proposal less than u, proposal is not accepted and the previuous x remains
x <- Xstar # if proposal is accepted the x* becomes the x
vec[i] <- x # save the new or old x into the vector.
}
vec #return the vector so it can be put on plot, to show the underlying distribution.
}
#lets try it out with 2000 samples
xs <- normm(2*10^3, 1)
hist(xs)
plot(xs, type = "l", col = "red")
xs <- normm(2*10^3, 1)
hist(xs)
plot(xs, type = "l", col = "red")
xs <- normm(2*10^2, 1)
hist(xs)
plot(xs, type = "l", col = "red")
normm<-function (Nsim, a)
{
vec <- vector("numeric", Nsim)
x <- 0
vec[1] <- x
N_acc = 0
for (i in 2:Nsim) {
innov <- runif(1, -a, a)
Xstar <- x + innov
aprob <- min(1, dnorm(Xstar)/dnorm(x))
u <- runif(1)
if (u < aprob) {
x <- Xstar
N_acc = N_acc + 1 # it is sufficient that we count the times the proposal is accepted
}
vec[i] <- x
}
return(list(vec, N_acc/Nsim)) #return the vector and the
}
results <- normm(10^3,1)
results[2]
results <- normm(10^3,0.1)
results[2]
results[2][1] +3
results <- normm(10^3, 1)
a <- 1
results[2][1] +3
results[2] +3
normm<-function (Nsim, a)
{
vec <- vector("numeric", Nsim)
x <- 0
vec[1] <- x
N_acc = 0
for (i in 2:Nsim) {
innov <- runif(1, -a, a)
Xstar <- x + innov
aprob <- min(1, dnorm(Xstar)/dnorm(x))
u <- runif(1)
if (u < aprob) {
x <- Xstar
N_acc = N_acc + 1 # it is sufficient that we count the times the proposal is accepted
}
vec[i] <- x
}
return(list(res = vec, acc = N_acc/Nsim)) #return the vector and the
}
results <- normm(10^3,1)
results[2]
results$acc
results$acc
library(dplyr)
results <- normm(10^3, 1)
a <- 1
results[2] +3
library(dplyr)
results <- normm(10^3, 1)
a <- 1
while(!between(results$acc, 0.25,0.35) ){
a = a + 0.1
results <- normm(10^3, a)
print(results[2])
}
library(dplyr)
results <- normm(10^3, 1)
a <- 1
r <- 0.30
v <- 0.03
while(!between(results$acc, r-v,r+v) ){
a = a + 0.1
results <- normm(10^3, a)
print(results[2])
}
library(dplyr)
results <- normm(10^3, 1)
a <- 1
r <- 0.30
v <- 0.02
while(!between(results$acc, r-v,r+v) ){
a = a + 0.1
results <- normm(10^3, a)
}
library(dplyr)
results <- normm(10^3, 1)
a <- 1
r <- 0.30
v <- 0.02
while(!between(results$acc, r-v,r+v) ){
a = a + 0.1
results <- normm(10^3, a)
}
print(a)
print(results$acc)
library(dplyr)
results <- normm(10^3, 1)
a <- 1
r <- 0.30
v <- 0.01
while(!between(results$acc, r-v,r+v) ){
a = a + 0.1
results <- normm(10^3, a)
}
print(a)
print(results$acc)
library(dplyr)
results <- normm(10^3, 1)
a <- 1
r <- 0.30
v <- 0.01
while(!between(results$acc, r-v,r+v) ){
a = a + 0.001
results <- normm(10^3, a)
}
print(a)
print(results$acc)
library(dplyr)
results <- normm(10^3, 1)
a <- 1
r <- 0.30
v <- 0.001
learning_rate = 0.001
while(!between(results$acc, r-v,r+v) ){
a = a + learning_rate
results <- normm(10^3, a)
}
print(a)
print(results$acc)
plot(results$res)
plot(results$res, type = "l", col = "red")
hist(results$res)
plot(results$res, type = "l", col = "red")
hist(results$res)
gammh<-function (Nsim, a, b)
{        mu <- a/b
sig <- sqrt(a/(b * b))
vec <- vector("numeric", Nsim)
x <- a/b
vec[1] <- x
for (i in 2:Nsim) {
can <- rnorm(1, mu, sig)
aprob <- min(1, (dgamma(can, a, b)/dgamma(x,
a, b))/(dnorm(can, mu, sig)/dnorm(x,mu, sig)))
u <- runif(1)
if (u < aprob)
x <- can
vec[i] <- x
}
vec
}
gammh(2*10^3, 0.1, 0.01)
results <- gammh(2*10^3, 0.1, 0.01)
plot(results, type = "l", col = "red")
results <- gammh(2*10^3, 0.1, 0.01)
plot(results, type = "l", col = "red")
hist(results)
results <- gammh(2*10^3, 1, 2)
plot(results, type = "l", col = "red")
hist(results)
housing <- read.csv("C:/Users/Aleksi/Desktop/KOulu/Simulation Methods/Exercises/exercise 4/housing.txt", sep="")
View(housing)
knitr::opts_chunk$set(echo = TRUE)
housing <- read.csv("C:/Users/Aleksi/Desktop/KOulu/Simulation Methods/Exercises/exercise 4/housing.txt", sep="")
head(housing, 6)
housing <- read.csv("C:/Users/Aleksi/Desktop/KOulu/Simulation Methods/Exercises/exercise 4/housing.txt", sep="")
head(housing, 6)
housing[,6]
housing[,7]
y=log(as.vector(housing[,7]))
dim(housing)
X=as.matrix(housing[,9:21])
X
y=log(as.vector(housing[,7]))
X=as.matrix(housing[,9:21])
housing <- read.csv("C:/Users/Aleksi/Desktop/KOulu/Simulation Methods/Exercises/exercise 4/housing.txt", sep="")
dim(housing)
head(housing, 6)
y=log(as.vector(housing[,7]))
X=as.matrix(housing[,9:21])
#This function inverses an matrix
inv=function(X){
EV=eigen(X)
EV$vector%*%diag(1/EV$values)%*%t(EV$vector)
}
# marginal density for gamma.
lpostw=function(gam,y,X,beta){
n=length(y)
qgam=sum(gam)
Xt1=cbind(rep(1,n),X[,which(gam==1)])
if (qgam!=0) P1=Xt1%*%inv(t(Xt1)%*%Xt1)%*%t(Xt1) else{
P1=matrix(0,n,n)}
-(qgam+1)/2*log(n+1)-n/2*log(t(y)%*%y-n/(n+1)*
t(y)%*%P1%*%y-1/(n+1)*t(beta)%*%t(cbind(rep(1,n),
X))%*%P1%*%cbind(rep(1,n),X)%*%beta)
}
#The Metropolis-Hasting algorithm itself
gocho=function(niter,y,X){
lga=dim(X)[2]
beta=lm(y~X)$coeff
gamma=matrix(0,nrow=niter,ncol=lga)
gamma[1,]=sample(c(0,1),lga,rep=T)
for (t in 1:(niter-1)){
j=sample(1:lga,1)
gam0=gam1=gamma[t,];gam1[j]=1-gam0[j]
pr=lpostw(gam0,y,X,beta)
pr=c(pr,lpostw(gam1,y,X,beta))
pr=exp(pr-max(pr))
gamma[t+1,]=gam0
if (sample(c(0,1),1,prob=pr))
gamma[t+1,]=gam1}
gamma
}
out <- gocho(25*10^3, y = y, X = X)
X
y
out <- gocho(25*10^3, y = y, X = X)
X=as.matrix(na.omit(housing[,9:21]))
y=log(as.vector(housing[,7]))
X=as.matrix(na.omit(housing[,9:21]))
out <- gocho(25*10^3, y = y, X = X)
y=log(as.vector(na.omit(housing[,7])))
X=as.matrix(na.omit(housing[,9:21]))
dim(y)
y
lenght(y)
length(y)
nrow(X)
length(y)
nrow(X)
out <- gocho(25*10^3, y = y, X = X)
out
plot(out)
hist(out)
out[:25000]
out[:2500]
out[2500]
out[25000]
View(out)
apply(out,2,mean)
table(out)
res <- data.frame(out)
summary(res)
summary(res)
table(res)
#The Metropolis-Hasting algorithm itself.
gocho=function(niter,y,X){
lga=dim(X)[2]
beta=lm(y~X)$coeff
gamma=matrix(0,nrow=niter,ncol=lga)
gamma[1,]=sample(c(0,1),lga,rep=T)
for (t in 1:(niter-1)){
j=sample(1:lga,1)
gam0=gam1=gamma[t,];gam1[j]=1-gam0[j]
pr=lpostw(gam0,y,X,beta)
pr=c(pr,lpostw(gam1,y,X,beta))
pr=exp(pr-max(pr))
gamma[t+1,]=gam0
if (sample(c(0,1),1,prob=pr))
gamma[t+1,]=gam1
}
gamma
}
out <- gocho(25*10^3, y = y, X = X)
ifelse(apply(out,2,mean) < 0.5,0, 1)
apply(out,2,mean)
ifelse(apply(out,2,mean) < 0.5,0, 1)
res <- data.frame(out)
summary(res)
#The Metropolis-Hasting algorithm itself.
gocho=function(niter,y,X){
lga=dim(X)[2]
beta=lm(y~X)$coeff
gamma=matrix(0,nrow=niter,ncol=lga)
gamma[1,]=sample(c(0,1),lga,rep=T)
for (t in 1:(niter-1)){
j=sample(1:lga,1)
gam0=gam1=gamma[t,];gam1[j]=1-gam0[j]
pr=lpostw(gam0,y,X,beta)
pr=c(pr,lpostw(gam1,y,X,beta))
pr=exp(pr-max(pr))
gamma[t+1,]=gam0
if (sample(c(0,1),1,prob=pr)){
gamma[t+1,]=gam1
}
}
gamma
}
```{r}
out <- gocho(25*10^3, y = y, X = X)
apply(out,2,mean)
ifelse(apply(out,2,mean) < 0.5,0, 1)
x
X
adm <- ifelse(apply(out,2,mean) < 0.5,0, 1)
adm
adm*X
head(adm*X,1)
X
adm*X
adm
X[amd]
X[adm]
head(X[adm],3)
X[,adm]
X[adm,]
adm <- ifelse(apply(out,2,mean) < 0.5,0, 1)
adm
adm
X[adm,]
X[adm]
X[adm,]
X[,adm]
X[adm,]
out[5000:]
out[5000:end]
out[5000:25000]
out[,5000:]
out[,5000:25000]
out[,5000:25000]
out
out[1]
out[1,2]
out
out[2,]
out[5000:25000,]
#5000 burn-in
out <- out[5000:25000,]
apply(out[5000:25000],2,mean)
apply(out,2,mean)
adm <- ifelse(apply(out,2,mean) < 0.5,0, 1)
adm
View(housing)
X[adm,]
knitr::opts_chunk$set(echo = TRUE)
#5000 burn-in
out <- out[5000:25000,]
#This function inverses an matrix
inv=function(X){
EV=eigen(X)
EV$vector%*%diag(1/EV$values)%*%t(EV$vector)
}
#log-marginal density of model gamma
lpostw=function(gam,y,X,beta){
n=length(y)
qgam=sum(gam)
Xt1=cbind(rep(1,n),X[,which(gam==1)])
if (qgam!=0) P1=Xt1%*%inv(t(Xt1)%*%Xt1)%*%t(Xt1) else{
P1=matrix(0,n,n)}
-(qgam+1)/2*log(n+1)-n/2*log(t(y)%*%y-n/(n+1)*
t(y)%*%P1%*%y-1/(n+1)*t(beta)%*%t(cbind(rep(1,n),
X))%*%P1%*%cbind(rep(1,n),X)%*%beta)
}
#The Metropolis-Hasting algorithm itself.
gocho=function(niter,y,X){
lga=dim(X)[2]
beta=lm(y~X)$coeff
gamma=matrix(0,nrow=niter,ncol=lga)
gamma[1,]=sample(c(0,1),lga,rep=T)
for (t in 1:(niter-1)){
j=sample(1:lga,1)
gam0=gam1=gamma[t,];gam1[j]=1-gam0[j]
pr=lpostw(gam0,y,X,beta)
pr=c(pr,lpostw(gam1,y,X,beta))
pr=exp(pr-max(pr))
gamma[t+1,]=gam0
if (sample(c(0,1),1,prob=pr)){
gamma[t+1,]=gam1
}
}
gamma
}
out <- gocho(25*10^3, y = y, X = X)
#5000 burn-in
out <- out[5000:25000,]
GA <- c(1,1,0,1,0,1,1,1,1,1,1,1,1)
MH <- c(1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1)
SA <- c(0,1,0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1)
GA <- c(1,1,0,1,0,1,1,1,1,1,1,1,1)
MH <- c(1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1)
SA <- c(0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1)
GA <- c(1, 1, 0, 1, 0, 1, 1, 1, 1, 1 ,1, 1, 1)
MH -SA
MH - GA
MH - SA
knitr::opts_chunk$set(echo = TRUE)
xs <- normm(2*10^3, 0.1)
xs <- normm(2*10^2, 1)
normm<-function (Nsim, a) # give the n. of simulations and the a rate. that is the acceptance rate.
{       vec <- vector("numeric", Nsim) # create an vector where the simulation results are stored
x <- 0 # Init value
vec[1] <- x # put init value to the vector that stores the resultsa
for (i in 2:Nsim) { # start the for loop that goes from 2 to the Nsim
innov <- runif(1, -a, a) # sample from uniform[-a, a]
Xstar <- x + innov # proposal plus random walk element innov
aprob <- min(1, dnorm(Xstar)/dnorm(x)) # aprov rate
u <- runif(1) # sample the minumum that must be cleared, so that the proposal is accepted.
if (u < aprob)  # if proposal less than u, proposal is not accepted and the previuous x remains
x <- Xstar # if proposal is accepted the x* becomes the x
vec[i] <- x # save the new or old x into the vector.
}
vec #return the vector so it can be put on plot, to show the underlying distribution.
}
#lets try it out with 2000 samples
xs <- normm(2*10^3, 1)
hist(xs)
plot(xs, type = "l", col = "red")
xs <- normm(2*10^2, 1)
hist(xs)
plot(xs, type = "l", col = "red")
xs <- normm(2*10^3, 0.1)
hist(xs)
plot(xs, type = "l", col = "red")
xs <- normm(2*10^2, 1)
hist(xs)
plot(xs, type = "l", col = "red")
xs <- normm(2*10^2, 0.1)
hist(xs)
plot(xs, type = "l", col = "red")
library(dplyr)
results <- normm(10^3, 1)
a <- 1
r <- 0.30
v <- 0.001
learning_rate = 0.001
while(!between(results$acc, r-v,r+v) ){
a = a + learning_rate
results <- normm(2*10^3, a)
}
library(dplyr)
results <- normm(10^3, 1)
a <- 1
r <- 0.30
v <- 0.001
learning_rate = 0.001
while(!between(results$acc, r-v,r+v) ){
a = a + learning_rate
results <- normm(2*10^3, a)
}
library(dplyr)
results <- normm(10^3, 1)
a <- 1
r <- 0.30
v <- 0.001
learning_rate = 0.001
while(!between(results$acc, r-v,r+v) ){
a = a + learning_rate
results <- normm(2*10^3, a)
}
normm<-function (Nsim, a)
{
vec <- vector("numeric", Nsim)
x <- 0
vec[1] <- x
N_acc = 0
for (i in 2:Nsim) {
innov <- runif(1, -a, a)
Xstar <- x + innov
aprob <- min(1, dnorm(Xstar)/dnorm(x))
u <- runif(1)
if (u < aprob) {
x <- Xstar
N_acc = N_acc + 1 # it is sufficient that we count the times the proposal is accepted
}
vec[i] <- x
}
return(list(res = vec, acc = N_acc/Nsim)) #return the vector and the acceptance rate. It is no the number of time the proposal was accepted divided by the proposal numbers that is the same as Nsim.
}
results <- normm(10^3,1)
results$acc
results <- normm(10^3,0.1)
results[2]
library(dplyr)
results <- normm(10^3, 1)
a <- 1
r <- 0.30
v <- 0.001
learning_rate = 0.001
while(!between(results$acc, r-v,r+v) ){
a = a + learning_rate
results <- normm(2*10^3, a)
}
print(a)
print(results$acc)
plot(results$res, type = "l", col = "red")
hist(results$res)
plot(results$res, type = "l", col = "red")
hist(results$res)
