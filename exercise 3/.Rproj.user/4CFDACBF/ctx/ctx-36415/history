y <- as.matrix(y)
f <- lm(y ~ run.vars)
run.step.bic <- BIC(f)
prob <- min(1,exp((run.bic - BIC(f))/tau[j]))
print(run.step.bic)
if(run.step.bic < run.bic){
run.current <- run.step
run.bic <- run.step.bic
}
if(rbinom(1,1,prob)){
run.current <- run.step
run.bic <- run.step.bic
}
if(run.step.bic < best.bic){ #ACCEPT IF BETTER THAN CURRENT BEST
run <- run.step
best.bic <- run.step.bic
}
bics <- c(bics,run.bic)
}
}
housing_data <- housing[7:21]
attach(housing_data)
summary(lm(MEDV~ ., data = housing_data))
housing <- read.csv("C:/Users/Aleksi/Desktop/KOulu/Simulation Methods/Exercises/exercise 3/housing.txt", sep="")
head(housing, 3)
housing_data <- housing[7:21]
attach(housing_data)
summary(lm(MEDV~ ., data = housing_data))
## DATA AND INITIAL VALUES
pred <- housing_data[-1:-2] #
y <- housing_data[1]
n <- nrow(housing_data)
ptot <- length(pred[1,]) # NUMBER OF PREDICTORS
m <- c(rep(60,5),rep(120,5),rep(220,5)) # THREE STAGE LENGTHS
tau.start <- 1
tau <- rep(tau.start,15)
bics <- NULL
set.seed(12345)
run <- rbinom(ptot,1,.5) # SAMPLE INDICATOR FOR PREDICTORS
run.current <- run
run.vars <- pred[,run.current==1] #SELECT PREDICTORS BASED ON INDICATORS
run.vars <- as.matrix(run.vars)
y <- as.matrix(y)
f <- lm(y ~ run.vars)# INITIAL LINEAR MODEL
#run.bic <- BIC(f)[2] # EXTRACT BIC
run.bic <- BIC(f) # EXTRACT BIC
best.bic <- run.bic
bics <- run.bic
for(j in 2:13){tau[j] <- 0.9*tau[j-1]} #TEMP SCHEDULE 15 LEVELS
## SIMULATED ANNEALING
for(j in 1:13){# RANDOMLY SELECTS A PREDICTOR TO ADD/REMOVE FROM THE MODEL
# AND ACCEPTS THE NEW MODEL IF IT IS BETTER OR WITH PROBABILITY prob
for(i in 1:m[j]){
pos <- sample(1:ptot,1)
run.step <- run.current
run.step[pos] <- !run.current[pos]
run.vars <- pred[,run.step==1]
run.vars <- as.matrix(run.vars)
y <- as.matrix(y)
f <- lm(y ~ run.vars)
run.step.bic <- BIC(f)
prob <- min(1,exp((run.bic - BIC(f))/tau[j]))
print(run.step.bic)
if(run.step.bic < run.bic){
run.current <- run.step
run.bic <- run.step.bic
}
if(rbinom(1,1,prob)){
run.current <- run.step
run.bic <- run.step.bic
}
if(run.step.bic < best.bic){ #ACCEPT IF BETTER THAN CURRENT BEST
run <- run.step
best.bic <- run.step.bic
}
bics <- c(bics,run.bic)
}
}
set.seed(12345)
run <- rbinom(ptot,1,.5) # SAMPLE INDICATOR FOR PREDICTORS
run.current <- run
run.vars <- pred[,run.current==1] #SELECT PREDICTORS BASED ON INDICATORS
run.vars <- as.matrix(run.vars)
y <- as.matrix(y)
f <- lm(y ~ run.vars)# INITIAL LINEAR MODEL
#run.bic <- BIC(f)[2] # EXTRACT BIC
run.bic <- BIC(f) # EXTRACT BIC
best.bic <- run.bic
bics <- run.bic
for(j in 2:13){tau[j] <- 0.9*tau[j-1]} #TEMP SCHEDULE 15 LEVELS
## SIMULATED ANNEALING
for(j in 1:13){# RANDOMLY SELECTS A PREDICTOR TO ADD/REMOVE FROM THE MODEL
# AND ACCEPTS THE NEW MODEL IF IT IS BETTER OR WITH PROBABILITY prob
for(i in 1:m[j]){
pos <- sample(1:ptot,1)
run.step <- run.current
run.step[pos] <- !run.current[pos]
run.vars <- pred[,run.step==1]
run.vars <- as.matrix(run.vars)
y <- as.matrix(y)
f <- lm(y ~ run.vars)
run.step.bic <- BIC(f)
prob <- min(1,exp((run.bic - BIC(f))/tau[j]))
print(run.step.bic)
if(run.step.bic < run.bic){
run.current <- run.step
run.bic <- run.step.bic
}
if(rbinom(1,1,prob)){
run.current <- run.step
run.bic <- run.step.bic
}
if(run.step.bic < best.bic){ #ACCEPT IF BETTER THAN CURRENT BEST
run <- run.step
best.bic <- run.step.bic
}
bics <- c(bics,run.bic)
}
}
bics
plot(bivs)
plot(bics)
plot(bics, type = "l")
install.package(GA)
install.packages(GA)
install.package(GA)
install.package(GA)
install.package(GA)
install.package(GA)
```{r}
```{r}
```{r}
mod <- lm(salary.log ~ .,data=pred)
```{r}
mod <- lm(salary.log ~ .,data=pred)
##dsad
###dsada
#dqa
mod <- lm(salary.log ~ .,data=pred)
mod <- lm(salary.log ~ .,data=pred)
run.vars <- as.matrix(run.vars)
y <- as.matrix(y)
run.vars <- as.matrix(run.vars)
y <- as.matrix(y)
mod <- lm(y ~ run.vars)# INITIAL LINEAR MODEL
#mod <- lm(salary.log ~ .,data=pred)
x <- model.matrix(mod)[, -1]
y <- model.response(model.frame(mod))
library(GA)
fitness <- function(string) {
inc <- which(string == 1)
X <- cbind(1, x[,inc])
mod <- lm.fit(X, y)
class(mod) <- "lm"
-extractAIC(mod)[2]
}
library(GA)
install.packages(GA)
install.packages(GA)
library(GA)
library(GA)
install.packages(GA)
install.packages("GA")
run.vars <- as.matrix(run.vars)
y <- as.matrix(y)
mod <- lm(y ~ run.vars)# INITIAL LINEAR MODEL
pred
pred <- as.matrix(pred)
y <- as.matrix(y)
mod <- lm(y ~ pred)# INITIAL LINEAR MODEL
y
pred
nrod
nrow(pred)
nrow(y)
#mod <- lm(y ~ pred)
mod <- lm(y ~ .,data=pred)
pred <- as.matrix(pred)
y <- as.matrix(y)
#mod <- lm(y ~ pred)
mod <- lm(y ~ .,data=pred)
mod <- lm(y ~ pred)
mod <- lm(y ~ pred,)
nrow(pred)
nrow(y)
nrow(pred)
## DATA AND INITIAL VALUES
pred <- housing_data[-1:-2] #
y <- housing_data[1]
nrow(pred)
n <- nrow(housing_data)
ptot <- length(pred[1,]) # NUMBER OF PREDICTORS
m <- c(rep(60,5),rep(120,5),rep(220,5)) # THREE STAGE LENGTHS
tau.start <- 1
tau <- rep(tau.start,15)
bics <- NULL
nrow(pred)
nrow(y)
pred <- housing_data[-1:-2] #
y <- housing_data[1]
pred <- as.matrix(pred)
y <- as.matrix(y)
mod <- lm(y ~ pred,)
#mod <- lm(y ~ .,data=pred)
x <- model.matrix(mod)[, -1]
y <- model.response(model.frame(mod))
install.packages(GA)
library(GA)
fitness <- function(string) {
inc <- which(string == 1)
X <- cbind(1, x[,inc])
mod <- lm.fit(X, y)
class(mod) <- "lm"
-extractAIC(mod)[2]
}
library(GA)
fitness <- function(string) {
inc <- which(string == 1)
X <- cbind(1, x[,inc])
mod <- lm.fit(X, y)
class(mod) <- "lm"
-extractAIC(mod)[2]
}
GA <- ga("binary", fitness = fitness, nBits = ncol(x),
names = colnames(x), monitor = plot)
GA <- ga("binary", fitness = fitness, nBits = ncol(x),
names = colnames(x))
ga[ยง]
ga[1]
GA[1]
GA$predZN
GA <- ga("binary", fitness = fitness, nBits = ncol(x),
names = colnames(x), monitor = plot)
GA <- ga("binary", fitness = fitness, nBits = ncol(x),
names = colnames(x), monitor = plot, verbose = 1)
GA <- ga("binary", fitness = fitness, nBits = ncol(x),
names = colnames(x), monitor = plot)
GA$predCRIM
GA$predCRIM
GA
summary(GA)
knitr::opts_chunk$set(echo = TRUE)
## DATA AND INITIAL VALUES
pred <- housing_data[-1:-2] #
y <- housing_data[1]
n <- nrow(housing_data)
ptot <- length(pred[1,]) # NUMBER OF PREDICTORS
m <- c(rep(60,5),rep(120,5),rep(220,5)) # THREE STAGE LENGTHS
tau.start <- 1
tau <- rep(tau.start,15)
bics <- NULL
set.seed(12345)
run <- rbinom(ptot,1,.5) # SAMPLE INDICATOR FOR PREDICTORS
run.current <- run
run.vars <- pred[,run.current==1] #SELECT PREDICTORS BASED ON INDICATORS
run.vars <- as.matrix(run.vars)
y <- as.matrix(y)
f <- lm(y ~ run.vars)# INITIAL LINEAR MODEL
#run.bic <- BIC(f)[2] # EXTRACT BIC
run.bic <- BIC(f) # EXTRACT BIC
best.bic <- run.bic
bics <- run.bic
for(j in 2:13){tau[j] <- 0.9*tau[j-1]} #TEMP SCHEDULE 15 LEVELS
set.seed(12345)
run <- rbinom(ptot,1,.5) # SAMPLE INDICATOR FOR PREDICTORS
run.current <- run
run.vars <- pred[,run.current==1] #SELECT PREDICTORS BASED ON INDICATORS
run.vars <- as.matrix(run.vars)
y <- as.matrix(y)
f <- lm(y ~ run.vars)# INITIAL LINEAR MODEL
#run.bic <- BIC(f)[2] # EXTRACT BIC
run.bic <- BIC(f) # EXTRACT BIC
best.bic <- run.bic
bics <- run.bic
for(j in 2:13){tau[j] <- 0.9*tau[j-1]} #TEMP SCHEDULE 15 LEVELS
## SIMULATED ANNEALING
for(j in 1:13){# RANDOMLY SELECTS A PREDICTOR TO ADD/REMOVE FROM THE MODEL
# AND ACCEPTS THE NEW MODEL IF IT IS BETTER OR WITH PROBABILITY prob
for(i in 1:m[j]){
pos <- sample(1:ptot,1)
run.step <- run.current
run.step[pos] <- !run.current[pos]
run.vars <- pred[,run.step==1]
run.vars <- as.matrix(run.vars)
y <- as.matrix(y)
f <- lm(y ~ run.vars)
run.step.bic <- BIC(f)
prob <- min(1,exp((run.bic - BIC(f))/tau[j]))
#print(run.step.bic)
if(run.step.bic < run.bic){
run.current <- run.step
run.bic <- run.step.bic
}
if(rbinom(1,1,prob)){
run.current <- run.step
run.bic <- run.step.bic
}
if(run.step.bic < best.bic){ #ACCEPT IF BETTER THAN CURRENT BEST
run <- run.step
best.bic <- run.step.bic
}
bics <- c(bics,run.bic)
}
}
plot(bics, type = "l")
plot(bics, type = "l", xlim = c(-5, 50))
pred <- housing_data[-1:-2] #
y <- housing_data[1]
pred <- as.matrix(pred)
y <- as.matrix(y)
mod <- lm(y ~ pred,)
#mod <- lm(y ~ .,data=pred)
x <- model.matrix(mod)[, -1]
y <- model.response(model.frame(mod))
library(GA)
fitness <- function(string) {
inc <- which(string == 1)
X <- cbind(1, x[,inc])
mod <- lm.fit(X, y)
class(mod) <- "lm"
-extractAIC(mod)[2]
}
GA <- ga("binary", fitness = fitness, nBits = ncol(x), names = colnames(x))
plot(GA)
plot(GA)
summary(GA)
plot(GA)
summary(GA)
optim(fn = H, par = p1, gr = "CG")
optim(fn = H, par = p2, gr = "CG")
optim(fn = H, par = p1, gr = "L-BFGS-B")
optim(fn = H, par = p2, gr = "L-BFGS-B")
#install.packages("numDeriv")
gridxy = as.matrix(expand.grid(x = seq(-5,5,0.1), y = seq(-5,5,0.1)))
likeval = apply(gridxy,1,H)
likedf = data.frame(gridxy,likeval)
colnames(likedf) = c("mu1","mu2","H")
library(numDeriv)
{#Initial parameters
diff = 1
iter = 1
x = c(0,-2)
xgdold = matrix(1,ncol=2,nrow=1000)
#Learning rate
lr = 0.001
#Gradient descent with automatic stopping
while (diff>10^-4){
xgdold[iter,] = x
grad = grad(func=H,x=x)
x = x - lr*grad
diff = sum(abs(x - xgdold[iter,]))/2
iter=iter+1
}
iter
#Collect parameter estimates in data frame
optpathgd = data.frame(xgdold[1:(iter-1),])
colnames(optpathgd) = c("mu1est1","mu2est1")
}
library(ggplot2)
#AGAIN WITH STARTING VALUES (1,1)
#Initial parameters
{
diff = 1
iter = 1
x = c(1,1)
xgdold = matrix(1,ncol=2,nrow=1000)
#Learning rate
lr = 0.001
#Gradient descent with automatic stopping
while (diff>10^-4){
xgdold[iter,] = x
grad = grad(func=H,x=x)
x = x - lr*grad
diff = sum(abs(x - xgdold[iter,]))/2
iter=iter+1
}
iter
#Collect parameter estimates in data frame
optpathgd1 = data.frame(xgdold[1:(iter-1),])
colnames(optpathgd1) = c("mu1est1","mu2est1")
}
library(ggplot2)
mixt_pl <- ggplot(likedf)
mixt_pl +
stat_contour(aes(x = mu1, y = mu2,
z = H,colour = after_stat(level)),bins=50)+
geom_path(data = optpathgd,aes(x = mu1est1,
y = mu2est1),colour="red")+
geom_path(data = optpathgd1,aes(x = mu1est1,
y = mu2est1),colour="blue")
```{r, warning=FALSE}
#install.packages("numDeriv")
gridxy = as.matrix(expand.grid(x = seq(-5,5,0.1), y = seq(-5,5,0.1)))
likeval = apply(gridxy,1,H)
likedf = data.frame(gridxy,likeval)
colnames(likedf) = c("mu1","mu2","H")
library(numDeriv)
{#Initial parameters
diff = 1
iter = 1
x = c(0,-2)
xgdold = matrix(1,ncol=2,nrow=1000)
#Learning rate
lr = 0.001
#Gradient descent with automatic stopping
while (diff>10^-4){
xgdold[iter,] = x
grad = grad(func=H,x=x)
x = x - lr*grad
diff = sum(abs(x - xgdold[iter,]))/2
iter=iter+1
}
iter
#Collect parameter estimates in data frame
optpathgd = data.frame(xgdold[1:(iter-1),])
colnames(optpathgd) = c("mu1est1","mu2est1")
}
library(ggplot2)
#AGAIN WITH STARTING VALUES (1,1)
#Initial parameters
{
diff = 1
iter = 1
x = c(1,1)
xgdold = matrix(1,ncol=2,nrow=1000)
#Learning rate
lr = 0.001
#Gradient descent with automatic stopping
while (diff>10^-4){
xgdold[iter,] = x
grad = grad(func=H,x=x)
x = x - lr*grad
diff = sum(abs(x - xgdold[iter,]))/2
iter=iter+1
}
iter
#Collect parameter estimates in data frame
optpathgd1 = data.frame(xgdold[1:(iter-1),])
colnames(optpathgd1) = c("mu1est1","mu2est1")
}
library(ggplot2)
mixt_pl <- ggplot(likedf)
mixt_pl +
stat_contour(aes(x = mu1, y = mu2,
z = H,colour = after_stat(level)),bins=50)+
geom_path(data = optpathgd,aes(x = mu1est1,
y = mu2est1),colour="red")+
geom_path(data = optpathgd1,aes(x = mu1est1,
y = mu2est1),colour="blue")
#install.packages("numDeriv")
gridxy = as.matrix(expand.grid(x = seq(-5,5,0.1), y = seq(-5,5,0.1)))
likeval = apply(gridxy,1,H)
likedf = data.frame(gridxy,likeval)
colnames(likedf) = c("mu1","mu2","H")
library(numDeriv)
{#Initial parameters
diff = 1
iter = 1
x = c(0,-2)
xgdold = matrix(1,ncol=2,nrow=1000)
#Learning rate
lr = 0.001
#Gradient descent with automatic stopping
while (diff>10^-4){
xgdold[iter,] = x
grad = grad(func=H,x=x)
x = x - lr*grad
diff = sum(abs(x - xgdold[iter,]))/2
iter=iter+1
}
iter
#Collect parameter estimates in data frame
optpathgd = data.frame(xgdold[1:(iter-1),])
colnames(optpathgd) = c("mu1est1","mu2est1")
}
library(ggplot2)
#AGAIN WITH STARTING VALUES (1,1)
#Initial parameters
{
diff = 1
iter = 1
x = c(1,1)
xgdold = matrix(1,ncol=2,nrow=1000)
#Learning rate
lr = 0.001
#Gradient descent with automatic stopping
while (diff>10^-4){
xgdold[iter,] = x
grad = grad(func=H,x=x)
x = x - lr*grad
diff = sum(abs(x - xgdold[iter,]))/2
iter=iter+1
}
iter
#Collect parameter estimates in data frame
optpathgd1 = data.frame(xgdold[1:(iter-1),])
colnames(optpathgd1) = c("mu1est1","mu2est1")
}
library(ggplot2)
mixt_pl <- ggplot(likedf)
mixt_pl +
stat_contour(aes(x = mu1, y = mu2,
z = H,colour = after_stat(level)),bins=50)+
geom_path(data = optpathgd,aes(x = mu1est1,
y = mu2est1),colour="red")+
geom_path(data = optpathgd1,aes(x = mu1est1,
y = mu2est1),colour="blue")
likedf
opthpath
optpathgd1
View(optpathgd1)
head(optpathgd1)
